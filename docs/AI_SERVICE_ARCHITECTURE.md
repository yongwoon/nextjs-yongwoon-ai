# AI ì„œë¹„ìŠ¤ êµ¬í˜„ ê°€ì´ë“œ

> ğŸ¤– **AI ê¸°ëŠ¥ êµ¬í˜„ì„ ìœ„í•œ ì‹¤ìš©ì ì¸ ê°œë°œ ê°€ì´ë“œ**

ì´ ë¬¸ì„œëŠ” í”„ë¡œì íŠ¸ì˜ AI ê¸°ëŠ¥ êµ¬í˜„ì— íŠ¹í™”ëœ ê°€ì´ë“œì…ë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì€ [`GETTING_STARTED.md`](./GETTING_STARTED.md)ë¥¼, ì „ì²´ ì•„í‚¤í…ì²˜ëŠ” [`DIRECTORY_ARCHITECTURE.md`](./DIRECTORY_ARCHITECTURE.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ¯ AI ì„œë¹„ìŠ¤ í•µì‹¬ ê¸°ëŠ¥

### ì§€ì›í•˜ëŠ” AI ê¸°ëŠ¥
- **ë‹¤ì¤‘ AI ëª¨ë¸**: OpenAI, Anthropic, Google AI í†µí•©
- **RAG ì‹œìŠ¤í…œ**: ë¬¸ì„œ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
- **í”„ë¡¬í”„íŠ¸ ìºì‹±**: Redis ê¸°ë°˜ ì„±ëŠ¥ ìµœì í™”
- **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: SSE ê¸°ë°˜ ì‹¤ì‹œê°„ ì‘ë‹µ
- **íŒŒì¼ ì²˜ë¦¬**: PDF, Word, ì´ë¯¸ì§€ ë¶„ì„
- **í…œí”Œë¦¿ ì‹œìŠ¤í…œ**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬

## ğŸ“¦ AI íŒ¨í‚¤ì§€ êµ¬ì„±

### í•µì‹¬ AI SDK
```json
{
  "@ai-sdk/openai": "^0.0.66",
  "@ai-sdk/anthropic": "^0.0.54",
  "@ai-sdk/google": "^0.0.52",
  "ai": "^4.0.36"
}
```

### RAG & ë²¡í„° ê²€ìƒ‰
```json
{
  "@langchain/core": "^0.3.29",
  "@langchain/openai": "^0.3.15",
  "@langchain/community": "^0.3.21",
  "@pinecone-database/pinecone": "^4.0.0"
}
```

### íŒŒì¼ ì²˜ë¦¬ & ìŠ¤í† ë¦¬ì§€
```json
{
  "@vercel/blob": "^0.27.0",
  "pdf-parse": "^1.1.1",
  "mammoth": "^1.8.0",
  "sharp": "^0.33.5"
}
```

### ìºì‹± & ì„±ëŠ¥
```json
{
  "ioredis": "^5.4.1",
  "@upstash/redis": "^1.36.1"
}
```

## ğŸš€ AI ëª¨ë¸ í†µí•©

### 1. AI ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„

```typescript
// src/infrastructure/ai-providers/openai/client.ts
import { openai } from '@ai-sdk/openai'
import { generateText, streamText } from 'ai'

export class OpenAIClient {
  private model: string

  constructor(model: string = 'gpt-4') {
    this.model = model
  }

  async generateResponse(prompt: string, options?: GenerateOptions) {
    const result = await generateText({
      model: openai(this.model),
      prompt,
      temperature: options?.temperature ?? 0.7,
      maxTokens: options?.maxTokens ?? 1000,
    })

    return result.text
  }

  async streamResponse(messages: Message[]) {
    const result = await streamText({
      model: openai(this.model),
      messages,
      onFinish: async ({ text, usage }) => {
        // ì‚¬ìš©ëŸ‰ ë¡œê¹…, DB ì €ì¥ ë“±
        await this.logUsage(usage)
      }
    })

    return result.toDataStreamResponse()
  }

  private async logUsage(usage: any) {
    // ì‚¬ìš©ëŸ‰ ì¶”ì  ë¡œì§
  }
}
```

### 2. ë‹¤ì¤‘ ëª¨ë¸ ê´€ë¦¬ì

```typescript
// src/domains/chat/services/ai-model-manager.ts
export class AIModelManager {
  private providers = new Map<string, AIProvider>()

  constructor() {
    this.providers.set('openai', new OpenAIClient())
    this.providers.set('anthropic', new AnthropicClient())
    this.providers.set('google', new GoogleAIClient())
  }

  async generateResponse(
    provider: string,
    prompt: string,
    options?: GenerateOptions
  ): Promise<string> {
    const client = this.providers.get(provider)
    if (!client) {
      throw new Error(`Unsupported AI provider: ${provider}`)
    }

    return await client.generateResponse(prompt, options)
  }

  async streamResponse(provider: string, messages: Message[]) {
    const client = this.providers.get(provider)
    if (!client) {
      throw new Error(`Unsupported AI provider: ${provider}`)
    }

    return await client.streamResponse(messages)
  }

  getAvailableProviders(): string[] {
    return Array.from(this.providers.keys())
  }
}
```

## ğŸ§  í”„ë¡¬í”„íŠ¸ ìºì‹± ì‹œìŠ¤í…œ

### 1. ìºì‹œ ê´€ë¦¬ì

```typescript
// src/domains/chat/services/prompt-cache.service.ts
import { createHash } from 'crypto'
import Redis from 'ioredis'

export class PromptCacheService {
  private redis: Redis

  constructor() {
    this.redis = new Redis(process.env.REDIS_URL!)
  }

  // í”„ë¡¬í”„íŠ¸ í•´ì‹œ ìƒì„±
  private generatePromptHash(prompt: string, model: string, options: any): string {
    const content = JSON.stringify({ prompt, model, options })
    return createHash('sha256').update(content).digest('hex')
  }

  // ìºì‹œëœ ì‘ë‹µ ì¡°íšŒ
  async getCachedResponse(
    prompt: string,
    model: string,
    options: any = {}
  ): Promise<string | null> {
    const hash = this.generatePromptHash(prompt, model, options)
    const cached = await this.redis.get(`prompt:${hash}`)

    if (cached) {
      // ìºì‹œ íˆíŠ¸ ë©”íŠ¸ë¦­ ê¸°ë¡
      await this.recordCacheHit(hash)
    }

    return cached
  }

  // ì‘ë‹µ ìºì‹œ ì €ì¥
  async setCachedResponse(
    prompt: string,
    model: string,
    response: string,
    options: any = {},
    ttl: number = 3600
  ): Promise<void> {
    const hash = this.generatePromptHash(prompt, model, options)
    await this.redis.setex(`prompt:${hash}`, ttl, response)
  }

  // ì‚¬ìš©ìë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ
  async getUserContext(userId: string): Promise<any> {
    const context = await this.redis.get(`user_context:${userId}`)
    return context ? JSON.parse(context) : null
  }

  async setUserContext(userId: string, context: any, ttl: number = 1800): Promise<void> {
    await this.redis.setex(
      `user_context:${userId}`,
      ttl,
      JSON.stringify(context)
    )
  }

  // ì„¸ì…˜ ê¸°ë°˜ ëŒ€í™” ê¸°ë¡
  async getSessionHistory(sessionId: string): Promise<Message[]> {
    const history = await this.redis.get(`session:${sessionId}`)
    return history ? JSON.parse(history) : []
  }

  async addToSessionHistory(sessionId: string, message: Message): Promise<void> {
    const history = await this.getSessionHistory(sessionId)
    history.push(message)

    // ìµœê·¼ 20ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
    const recentHistory = history.slice(-20)

    await this.redis.setex(
      `session:${sessionId}`,
      3600, // 1ì‹œê°„
      JSON.stringify(recentHistory)
    )
  }

  private async recordCacheHit(hash: string): Promise<void> {
    await this.redis.incr(`cache_hits:${hash}`)
  }
}
```

### 2. ìºì‹œ ì „ëµ êµ¬í˜„

```typescript
// src/domains/chat/services/application-services/chat.service.ts
export class ChatService {
  constructor(
    private aiModelManager: AIModelManager,
    private promptCache: PromptCacheService,
    private ragService: RAGService
  ) {}

  async generateResponse(request: ChatRequest): Promise<ChatResponse> {
    const { prompt, model, userId, sessionId, useRAG } = request

    // 1. RAG ì»¨í…ìŠ¤íŠ¸ ìƒì„± (í•„ìš”ì‹œ)
    let enhancedPrompt = prompt
    if (useRAG) {
      const context = await this.ragService.getRelevantContext(prompt)
      enhancedPrompt = this.buildRAGPrompt(prompt, context)
    }

    // 2. ìºì‹œ í™•ì¸
    const cachedResponse = await this.promptCache.getCachedResponse(
      enhancedPrompt,
      model,
      request.options
    )

    if (cachedResponse) {
      return {
        response: cachedResponse,
        cached: true,
        usage: null
      }
    }

    // 3. AI ëª¨ë¸ í˜¸ì¶œ
    const response = await this.aiModelManager.generateResponse(
      model,
      enhancedPrompt,
      request.options
    )

    // 4. ì‘ë‹µ ìºì‹œ ì €ì¥
    await this.promptCache.setCachedResponse(
      enhancedPrompt,
      model,
      response,
      request.options
    )

    // 5. ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    await this.promptCache.addToSessionHistory(sessionId, {
      role: 'user',
      content: prompt
    })
    await this.promptCache.addToSessionHistory(sessionId, {
      role: 'assistant',
      content: response
    })

    return {
      response,
      cached: false,
      usage: null // ì‹¤ì œ ì‚¬ìš©ëŸ‰ ì •ë³´
    }
  }

  private buildRAGPrompt(query: string, context: string[]): string {
    return `
ì»¨í…ìŠ¤íŠ¸ ì •ë³´:
${context.join('\n\n')}

ì§ˆë¬¸: ${query}

ìœ„ì˜ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    `.trim()
  }
}
```

## ğŸ” RAG ì‹œìŠ¤í…œ êµ¬í˜„

### 1. ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬

```typescript
// src/domains/documents/services/vector-store.service.ts
import { PineconeStore } from '@langchain/pinecone'
import { OpenAIEmbeddings } from '@langchain/openai'
import { Pinecone } from '@pinecone-database/pinecone'

export class VectorStoreService {
  private pinecone: Pinecone
  private vectorStore: PineconeStore
  private embeddings: OpenAIEmbeddings

  constructor() {
    this.pinecone = new Pinecone({
      apiKey: process.env.PINECONE_API_KEY!
    })

    this.embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY!
    })

    this.initializeVectorStore()
  }

  private async initializeVectorStore() {
    const index = this.pinecone.Index(process.env.PINECONE_INDEX_NAME!)

    this.vectorStore = await PineconeStore.fromExistingIndex(
      this.embeddings,
      { pineconeIndex: index }
    )
  }

  // ë¬¸ì„œ ì„ë² ë”© ë° ì €ì¥
  async addDocument(
    content: string,
    metadata: DocumentMetadata
  ): Promise<void> {
    const chunks = this.chunkDocument(content)

    const documents = chunks.map((chunk, index) => ({
      pageContent: chunk,
      metadata: {
        ...metadata,
        chunkIndex: index,
        chunkCount: chunks.length
      }
    }))

    await this.vectorStore.addDocuments(documents)
  }

  // ìœ ì‚¬ë„ ê²€ìƒ‰
  async searchSimilarDocuments(
    query: string,
    k: number = 5,
    filter?: Record<string, any>
  ): Promise<SearchResult[]> {
    const results = await this.vectorStore.similaritySearchWithScore(
      query,
      k,
      filter
    )

    return results.map(([doc, score]) => ({
      content: doc.pageContent,
      metadata: doc.metadata,
      similarity: score
    }))
  }

  // ë¬¸ì„œ ì²­í‚¹
  private chunkDocument(content: string, chunkSize: number = 1000): string[] {
    const chunks: string[] = []
    const sentences = content.split(/[.!?]+/)

    let currentChunk = ''

    for (const sentence of sentences) {
      if (currentChunk.length + sentence.length > chunkSize) {
        if (currentChunk) {
          chunks.push(currentChunk.trim())
          currentChunk = ''
        }
      }
      currentChunk += sentence + '. '
    }

    if (currentChunk) {
      chunks.push(currentChunk.trim())
    }

    return chunks
  }

  // ë°°ì¹˜ ì„ë² ë”© ìƒì„±
  async batchAddDocuments(documents: DocumentInput[]): Promise<void> {
    const batchSize = 10

    for (let i = 0; i < documents.length; i += batchSize) {
      const batch = documents.slice(i, i + batchSize)

      await Promise.all(
        batch.map(doc => this.addDocument(doc.content, doc.metadata))
      )

      // API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
      await new Promise(resolve => setTimeout(resolve, 1000))
    }
  }
}
```

### 2. RAG ì„œë¹„ìŠ¤

```typescript
// src/domains/chat/services/rag.service.ts
export class RAGService {
  constructor(
    private vectorStore: VectorStoreService,
    private documentService: DocumentService
  ) {}

  async getRelevantContext(
    query: string,
    userId: string,
    maxChunks: number = 5
  ): Promise<string[]> {
    // ì‚¬ìš©ìì˜ ë¬¸ì„œë§Œ ê²€ìƒ‰
    const filter = { userId }

    const results = await this.vectorStore.searchSimilarDocuments(
      query,
      maxChunks,
      filter
    )

    // ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©
    const relevantResults = results.filter(result => result.similarity > 0.7)

    return relevantResults.map(result => result.content)
  }

  async enhancePromptWithContext(
    prompt: string,
    userId: string
  ): Promise<string> {
    const context = await this.getRelevantContext(prompt, userId)

    if (context.length === 0) {
      return prompt
    }

    return `
ì°¸ê³  ë¬¸ì„œ:
${context.map((chunk, index) => `${index + 1}. ${chunk}`).join('\n\n')}

ì§ˆë¬¸: ${prompt}

ìœ„ì˜ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ì— ì‚¬ìš©í•œ ë¬¸ì„œì˜ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”.
    `.trim()
  }

  // ë¬¸ì„œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
  async calculateRelevanceScore(
    query: string,
    documentId: string
  ): Promise<number> {
    const document = await this.documentService.getDocument(documentId)
    if (!document) return 0

    const results = await this.vectorStore.searchSimilarDocuments(
      query,
      10,
      { documentId }
    )

    if (results.length === 0) return 0

    // í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜ ë°˜í™˜
    const avgScore = results.reduce((sum, result) => sum + result.similarity, 0) / results.length
    return avgScore
  }
}
```

## ğŸ“ íŒŒì¼ ì²˜ë¦¬ ì‹œìŠ¤í…œ

### 1. ë¬¸ì„œ íŒŒì„œ

```typescript
// src/domains/documents/services/document-parser.service.ts
import pdf from 'pdf-parse'
import mammoth from 'mammoth'
import sharp from 'sharp'

export class DocumentParserService {
  async parseDocument(file: File): Promise<ParsedDocument> {
    const buffer = await file.arrayBuffer()
    const uint8Array = new Uint8Array(buffer)

    switch (file.type) {
      case 'application/pdf':
        return await this.parsePDF(uint8Array)

      case 'application/vnd.openxmlformats-officedocument.wordprocessingml.document':
        return await this.parseWord(uint8Array)

      case 'text/plain':
        return await this.parseText(uint8Array)

      case 'image/jpeg':
      case 'image/png':
        return await this.parseImage(uint8Array)

      default:
        throw new Error(`Unsupported file type: ${file.type}`)
    }
  }

  private async parsePDF(buffer: Uint8Array): Promise<ParsedDocument> {
    const data = await pdf(buffer)

    return {
      content: data.text,
      metadata: {
        pages: data.numpages,
        title: data.info?.Title || '',
        author: data.info?.Author || '',
        creationDate: data.info?.CreationDate
      }
    }
  }

  private async parseWord(buffer: Uint8Array): Promise<ParsedDocument> {
    const result = await mammoth.extractRawText({ buffer })

    return {
      content: result.value,
      metadata: {
        warnings: result.messages
      }
    }
  }

  private async parseText(buffer: Uint8Array): Promise<ParsedDocument> {
    const content = new TextDecoder().decode(buffer)

    return {
      content,
      metadata: {
        encoding: 'utf-8',
        size: buffer.length
      }
    }
  }

  private async parseImage(buffer: Uint8Array): Promise<ParsedDocument> {
    // OCR ì²˜ë¦¬ (Tesseract.js ë“± ì‚¬ìš©)
    // í˜„ì¬ëŠ” ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ì¶œ
    const image = sharp(buffer)
    const metadata = await image.metadata()

    return {
      content: '', // OCR ê²°ê³¼ê°€ ë“¤ì–´ê°ˆ ìë¦¬
      metadata: {
        width: metadata.width,
        height: metadata.height,
        format: metadata.format,
        size: metadata.size
      }
    }
  }

  // ë¬¸ì„œ ì „ì²˜ë¦¬
  async preprocessDocument(content: string): Promise<string> {
    return content
      .replace(/\s+/g, ' ') // ì—°ì† ê³µë°± ì œê±°
      .replace(/\n{3,}/g, '\n\n') // ì—°ì† ì¤„ë°”ê¿ˆ ì •ë¦¬
      .trim()
  }
}
```

### 2. ë¬¸ì„œ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°

```typescript
// src/domains/documents/services/document-processing.service.ts
export class DocumentProcessingService {
  constructor(
    private parser: DocumentParserService,
    private vectorStore: VectorStoreService,
    private storage: StorageService,
    private eventBus: EventBus
  ) {}

  async processDocument(
    file: File,
    userId: string
  ): Promise<ProcessedDocument> {
    try {
      // 1. íŒŒì¼ ì €ì¥
      const fileUrl = await this.storage.uploadFile(file, userId)

      // 2. ë¬¸ì„œ íŒŒì‹±
      const parsed = await this.parser.parseDocument(file)

      // 3. ì „ì²˜ë¦¬
      const processedContent = await this.parser.preprocessDocument(parsed.content)

      // 4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
      const document = await this.saveDocument({
        userId,
        filename: file.name,
        fileUrl,
        content: processedContent,
        metadata: parsed.metadata,
        contentType: file.type,
        size: file.size
      })

      // 5. ë²¡í„° ì„ë² ë”© ìƒì„± (ë¹„ë™ê¸°)
      this.generateEmbeddings(document.id, processedContent, {
        userId,
        documentId: document.id,
        filename: file.name
      })

      // 6. ì´ë²¤íŠ¸ ë°œí–‰
      await this.eventBus.publish(
        new DocumentProcessedEvent(document.id, userId)
      )

      return document
    } catch (error) {
      await this.eventBus.publish(
        new DocumentProcessingFailedEvent(file.name, userId, error.message)
      )
      throw error
    }
  }

  private async generateEmbeddings(
    documentId: string,
    content: string,
    metadata: any
  ): Promise<void> {
    try {
      await this.vectorStore.addDocument(content, metadata)

      // ì„ë² ë”© ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
      await this.updateDocumentStatus(documentId, 'embedded')

      await this.eventBus.publish(
        new DocumentEmbeddedEvent(documentId)
      )
    } catch (error) {
      await this.updateDocumentStatus(documentId, 'embedding_failed')

      await this.eventBus.publish(
        new DocumentEmbeddingFailedEvent(documentId, error.message)
      )
    }
  }

  private async saveDocument(data: CreateDocumentData): Promise<Document> {
    // ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë¡œì§
    return await this.documentRepository.create(data)
  }

  private async updateDocumentStatus(
    documentId: string,
    status: DocumentStatus
  ): Promise<void> {
    await this.documentRepository.updateStatus(documentId, status)
  }
}
```

## ğŸ¨ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‹œìŠ¤í…œ

### 1. í…œí”Œë¦¿ ê´€ë¦¬

```typescript
// src/domains/templates/services/prompt-template.service.ts
export class PromptTemplateService {
  constructor(
    private templateRepository: PromptTemplateRepository
  ) {}

  async createTemplate(data: CreateTemplateData): Promise<PromptTemplate> {
    const template = new PromptTemplate({
      name: data.name,
      content: data.content,
      variables: this.extractVariables(data.content),
      category: data.category,
      isPublic: data.isPublic || false,
      userId: data.userId
    })

    return await this.templateRepository.save(template)
  }

  async renderTemplate(
    templateId: string,
    variables: Record<string, string>
  ): Promise<string> {
    const template = await this.templateRepository.findById(templateId)
    if (!template) {
      throw new Error('Template not found')
    }

    return this.interpolateVariables(template.content, variables)
  }

  // í…œí”Œë¦¿ì—ì„œ ë³€ìˆ˜ ì¶”ì¶œ
  private extractVariables(content: string): string[] {
    const variableRegex = /\{\{(\w+)\}\}/g
    const variables: string[] = []
    let match

    while ((match = variableRegex.exec(content)) !== null) {
      if (!variables.includes(match[1])) {
        variables.push(match[1])
      }
    }

    return variables
  }

  // ë³€ìˆ˜ ì¹˜í™˜
  private interpolateVariables(
    content: string,
    variables: Record<string, string>
  ): string {
    return content.replace(/\{\{(\w+)\}\}/g, (match, variable) => {
      return variables[variable] || match
    })
  }

  // ì¹´í…Œê³ ë¦¬ë³„ í…œí”Œë¦¿ ì¡°íšŒ
  async getTemplatesByCategory(
    category: string,
    userId?: string
  ): Promise<PromptTemplate[]> {
    return await this.templateRepository.findByCategory(category, userId)
  }

  // ì¸ê¸° í…œí”Œë¦¿ ì¡°íšŒ
  async getPopularTemplates(limit: number = 10): Promise<PromptTemplate[]> {
    return await this.templateRepository.findPopular(limit)
  }
}
```

### 2. í…œí”Œë¦¿ ì˜ˆì‹œ

```typescript
// src/domains/templates/data/default-templates.ts
export const DEFAULT_TEMPLATES = [
  {
    name: 'ì½”ë“œ ë¦¬ë·°',
    category: 'development',
    content: `
ë‹¤ìŒ {{language}} ì½”ë“œë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”:

\`\`\`{{language}}
{{code}}
\`\`\`

ë‹¤ìŒ ê´€ì ì—ì„œ ê²€í† í•´ì£¼ì„¸ìš”:
- ì½”ë“œ í’ˆì§ˆ
- ì„±ëŠ¥ ìµœì í™”
- ë³´ì•ˆ ì´ìŠˆ
- ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
    `.trim(),
    variables: ['language', 'code']
  },

  {
    name: 'ë¬¸ì„œ ìš”ì•½',
    category: 'productivity',
    content: `
ë‹¤ìŒ ë¬¸ì„œë¥¼ {{length}} ê¸¸ì´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{{document}}

ìš”ì•½ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
- í•µì‹¬ ë‚´ìš©
- ì£¼ìš” ê²°ë¡ 
- ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸
    `.trim(),
    variables: ['length', 'document']
  },

  {
    name: 'ì´ë©”ì¼ ì‘ì„±',
    category: 'communication',
    content: `
{{recipient}}ì—ê²Œ ë³´ë‚¼ {{tone}} í†¤ì˜ ì´ë©”ì¼ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ëª©ì : {{purpose}}
ì£¼ìš” ë‚´ìš©: {{content}}

ì´ë©”ì¼ êµ¬ì¡°:
- ì ì ˆí•œ ì¸ì‚¬ë§
- ëª…í™•í•œ ë³¸ë¬¸
- ì •ì¤‘í•œ ë§ˆë¬´ë¦¬
    `.trim(),
    variables: ['recipient', 'tone', 'purpose', 'content']
  }
]
```

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### 1. AI ì‚¬ìš©ëŸ‰ ì¶”ì 

```typescript
// src/domains/analytics/services/ai-usage-tracker.service.ts
export class AIUsageTrackerService {
  constructor(
    private metricsService: MetricsService,
    private usageRepository: UsageRepository
  ) {}

  async trackAPICall(data: APICallData): Promise<void> {
    // ë©”íŠ¸ë¦­ ê¸°ë¡
    this.metricsService.recordCounter('ai.api.calls', 1, {
      provider: data.provider,
      model: data.model,
      userId: data.userId
    })

    this.metricsService.recordDuration('ai.api.latency', data.latency, {
      provider: data.provider,
      model: data.model
    })

    // ì‚¬ìš©ëŸ‰ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    await this.usageRepository.create({
      userId: data.userId,
      provider: data.provider,
      model: data.model,
      inputTokens: data.inputTokens,
      outputTokens: data.outputTokens,
      cost: this.calculateCost(data),
      latency: data.latency,
      timestamp: new Date()
    })
  }

  async getUserUsage(
    userId: string,
    period: 'day' | 'week' | 'month'
  ): Promise<UsageSummary> {
    const startDate = this.getStartDate(period)
    const usage = await this.usageRepository.findByUserAndPeriod(userId, startDate)

    return {
      totalCalls: usage.length,
      totalTokens: usage.reduce((sum, u) => sum + u.inputTokens + u.outputTokens, 0),
      totalCost: usage.reduce((sum, u) => sum + u.cost, 0),
      averageLatency: usage.reduce((sum, u) => sum + u.latency, 0) / usage.length,
      byProvider: this.groupByProvider(usage)
    }
  }

  private calculateCost(data: APICallData): number {
    // ì œê³µìë³„ í† í° ê°€ê²© ê³„ì‚°
    const pricing = {
      openai: {
        'gpt-4': { input: 0.03, output: 0.06 }, // per 1K tokens
        'gpt-3.5-turbo': { input: 0.001, output: 0.002 }
      },
      anthropic: {
        'claude-3': { input: 0.015, output: 0.075 }
      }
    }

    const modelPricing = pricing[data.provider]?.[data.model]
    if (!modelPricing) return 0

    const inputCost = (data.inputTokens / 1000) * modelPricing.input
    const outputCost = (data.outputTokens / 1000) * modelPricing.output

    return inputCost + outputCost
  }
}
```

### 2. ìºì‹œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```typescript
// src/domains/analytics/services/cache-analytics.service.ts
export class CacheAnalyticsService {
  constructor(
    private redis: Redis,
    private metricsService: MetricsService
  ) {}

  async getCacheStats(): Promise<CacheStats> {
    const info = await this.redis.info('stats')
    const keyspaceInfo = await this.redis.info('keyspace')

    return {
      hitRate: this.calculateHitRate(info),
      totalKeys: this.getTotalKeys(keyspaceInfo),
      memoryUsage: await this.getMemoryUsage(),
      topKeys: await this.getTopKeys()
    }
  }

  async recordCacheOperation(
    operation: 'hit' | 'miss' | 'set',
    key: string,
    latency?: number
  ): Promise<void> {
    this.metricsService.recordCounter(`cache.${operation}`, 1, {
      keyType: this.getKeyType(key)
    })

    if (latency) {
      this.metricsService.recordDuration(`cache.${operation}.latency`, latency)
    }
  }

  private getKeyType(key: string): string {
    if (key.startsWith('prompt:')) return 'prompt'
    if (key.startsWith('session:')) return 'session'
    if (key.startsWith('user_context:')) return 'user_context'
    return 'other'
  }

  private async getTopKeys(): Promise<Array<{ key: string, ttl: number }>> {
    const keys = await this.redis.keys('*')
    const keyData = await Promise.all(
      keys.slice(0, 100).map(async key => ({
        key,
        ttl: await this.redis.ttl(key)
      }))
    )

    return keyData.sort((a, b) => b.ttl - a.ttl).slice(0, 10)
  }
}
```

## ğŸ”§ AI ì„œë¹„ìŠ¤ ì „ìš© í™˜ê²½ ë³€ìˆ˜

```env
# AI ëª¨ë¸ API í‚¤
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_AI_API_KEY=...

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
PINECONE_API_KEY=...
PINECONE_INDEX_NAME=yongwoon-ai
PINECONE_ENVIRONMENT=us-west1-gcp

# AI ì„œë¹„ìŠ¤ ì„¤ì •
DEFAULT_AI_PROVIDER=openai
DEFAULT_MODEL=gpt-4
MAX_TOKENS=2000
TEMPERATURE=0.7

# RAG ì„¤ì •
EMBEDDING_MODEL=text-embedding-ada-002
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
MAX_CONTEXT_CHUNKS=5

# ì„±ëŠ¥ ì„¤ì •
ENABLE_PROMPT_CACHING=true
ENABLE_RAG=true
ENABLE_USAGE_TRACKING=true
```

## ğŸš€ ë°°í¬ ë° ìµœì í™” ê³ ë ¤ì‚¬í•­

### 1. AI API ë¹„ìš© ìµœì í™”
- í”„ë¡¬í”„íŠ¸ ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
- í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ì œí•œ
- ëª¨ë¸ë³„ ë¹„ìš© íš¨ìœ¨ì„± ë¶„ì„

### 2. ì„±ëŠ¥ ìµœì í™”
- ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤ ìµœì í™”
- ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ ì„ë² ë”© ìƒì„±
- CDNì„ í†µí•œ ì •ì  ë¦¬ì†ŒìŠ¤ ìºì‹±

### 3. í™•ì¥ì„± ê³ ë ¤
- ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ ì¤€ë¹„
- ë¡œë“œ ë°¸ëŸ°ì‹± ë° ì˜¤í†  ìŠ¤ì¼€ì¼ë§
- ë°ì´í„°ë² ì´ìŠ¤ ìƒ¤ë”© ì „ëµ

ì´ ê°€ì´ë“œë¥¼ í†µí•´ AI ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ê¸°ëŠ¥ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ê¸°ëŠ¥ì€ ë…ë¦½ì ìœ¼ë¡œ ê°œë°œí•˜ê³  í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.